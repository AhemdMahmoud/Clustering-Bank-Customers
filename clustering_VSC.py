# -*- coding: utf-8 -*-
"""Clustering-Bank-Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sXvoUvsdDMVbNomkNjYgbk-nOd5wPX6t

# Project Objective:

### This project aims to categorize customers of bank accounts using advanced unsupervised learning techniques. By leveraging various methodologies and tools, we aim to uncover meaningful patterns within the data and effectively segment customers into distinct categories.

# üåü Business Understanding:
### This case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables

# Data Dictionary for Credit Card dataset :-


| Column Name                     | Description                                                                                                               |
|---------------------------------|---------------------------------------------------------------------------------------------------------------------------|
| CUSTID                          | Customer ID (Categorical)                                                                                                |
| BALANCE                         | Remaining balance amount in their account for purchases                                                                   |
| BALANCEFREQUENCY               | How frequently the balance is updated, scored between 0 and 1 (1 = frequently updated, 0 = not frequently updated)      |
| PURCHASES                       | Amount of purchases made from the account                                                                                 |
| ONEOFFPURCHASES                 | Maximum purchase amount done in one-go                                                                                    |
| INSTALLMENTSPURCHASES           | Amount of purchases done in installment                                                                                   |
| CASHADVANCE                     | Cash in advance given by the user                                                                                         |
| PURCHASESFREQUENCY              | How frequently purchases are being made, scored between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)|
| ONEOFFPURCHASESFREQUENCY        | How frequently one-off purchases are being made, scored between 0 and 1 (1 = frequently made, 0 = not frequently made)   |
| PURCHASESINSTALLMENTSFREQUENCY  | How frequently installment purchases are being made, scored between 0 and 1 (1 = frequently done, 0 = not frequently done)|
| CASHADVANCEFREQUENCY            | How frequently cash in advance is being taken                                                                             |
| CASHADVANCETRX                  | Number of transactions made with "Cash Advance"                                                                           |
| PURCHASESTRX                    | Number of purchase transactions                                                                                           |
| CREDITLIMIT                     | Credit limit for the user                                                                                                 |
| PAYMENTS                        | Amount of payments done by the user                                                                                       |
| MINIMUM_PAYMENTS                | Minimum amount of payments done by the user                                                                               |
| PRCFULLPAYMENT                  | Percentage of full payment paid by the user                                                                               |
| TENURE                          | Duration of credit card service for the user                                                                              |

# üìö Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import  sklearn.metrics as metrics
from sklearn.cluster import KMeans

from sklearn.preprocessing import RobustScaler

from scipy.stats import skew

import warnings
warnings.filterwarnings("ignore")

"""# ‚è≥ Loading Dataset"""

df=pd.read_csv("/content/drive/MyDrive/CC GENERAL.csv")
df.head(5)

"""# üß† Basic Understaning of Data"""

list_of_numerics=df.select_dtypes(include=['float','int']).columns
types= df.dtypes
missing= round((df.isnull().sum()/df.shape[0]),3)*100
overview= df.apply(lambda x: [round(x.min()),
                                 round(x.max()),
                                 round(x.mean()),
                                 round(x.quantile(0.5))] if x.name in list_of_numerics else x.unique())

outliers= df.apply(lambda x: sum(
                                 (x<(x.quantile(0.25)-1.5*(x.quantile(0.75)-x.quantile(0.25))))|
                                 (x>(x.quantile(0.75)+1.5*(x.quantile(0.75)-x.quantile(0.25))))
                                 if x.name in list_of_numerics else ''))

explo = pd.DataFrame({'Types': types,
                      'Missing%': missing,
                      'Overview': overview,
                      'Outliers': outliers}).sort_values(by=['Missing%','Types'],ascending=False)
explo.transpose()

df.describe()

"""# üßπ Data Preprocessing Part-1:"""

df=df.drop(columns="CUST_ID")

columns=df.columns

# assign based on unique value
low_cols = ['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY','ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',
       'CASH_ADVANCE_FREQUENCY', 'PRC_FULL_PAYMENT']

medium_cols = ['CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'TENURE']
merge=low_cols+medium_cols
other_cols = list(set(columns) - set(merge))

"""# üìä Exploatory Data Analysis EDA"""

def pick_random_color():
  return '#'+str(np.random.randint(100000, 999999))
def plot_multi_histplot(columns_list,bins):
  color=pick_random_color()
  plt.figure(figsize=(15,10))
  for idx,column in enumerate(columns_list):
      plt.subplot(3,2,idx+1)
      sns.histplot(x=column, data=df,bins=bins,kde=True,color=color)
      plt.title(f"{column} Distribusion")
      plt.ylabel('Frequency')
      plt.xlabel('Value')
      plt.grid(True)
      plt.tight_layout()

plot_multi_histplot(low_cols,bins=5)

"""### Since most of the expenses are 0 so the values with higher expenses are kind of outliers in our data
### üí° All the  features distribution is Right-Skewed

Insights from the previous plots:

- Regarding purchase frequency, there appear to be three distinct categories:
    - Approximately 3,100 individuals frequently make purchases.
    - About 3,500 individuals make purchases rarely.
    - Around 2,500 individuals make purchases moderately.

- Analysis of one-off purchase frequency reveals:
    - Roughly 7,300 individuals make one-off purchases sparingly, while approximately 1,000 individuals make such purchases frequently.

- Examination of balance frequency indicates that the majority, approximately 7,500 individuals, experience rapid balance changes, suggesting potential challenges in money management.

- Only around 2,000 individuals opt for immediate installment payments, while about 5,000 individuals choose to make installment payments regularly.

- A significant portion, around 7,500 customers, engage in cash advances.

<!-- - Analysis of the percentage of full payments reveals that approximately 7,000 individuals have outstanding balances, while only about 800 have paid off their balances in full. -->
"""

plot_multi_histplot(medium_cols,bins=10)

"""## Insights from the previous plots:
- Analysis of cash advance transactions reveals that approximately 8,000 individuals engage in transactions with cash advances fewer than 20 times.
- Analysis of purchase transactions indicates that around 8,000 individuals make transactions infrequently, typically fewer than 25 times.
- Examination of tenure data shows that roughly 7,500 users have utilized the credit card service for 12 months.

"""

plot_multi_histplot(other_cols,bins=7)

"""- Most users, approximately 8700, make installment purchases less than $2500,

  while only 230 users make purchases exceeding $2500.

- The mean balance across all accounts is approximately $1564.47.

- Some accounts have a balance of $0, indicating accounts with no remaining balance.

### check outliers & skewness
"""

skewness = df[columns].skew().sort_values()

plt.figure(figsize=(14,6))
sns.barplot(x=skewness.index, y=skewness, palette=sns.color_palette("Reds",19))
for i, v in enumerate(skewness):
  plt.text(i, v, f"{v:.1f}", ha="center", va="bottom",size=15,fontweight="black")

plt.ylabel("Skewness")
plt.xlabel("Columns")
plt.xticks(rotation=90)
plt.title("Skewness of Continous Numerical Columns",fontweight="black",size=20,pad=10)
plt.tight_layout()
plt.show()

"""- **Data Type Verification:**
  - The data types are confirmed to be correct.

- **Missing Values Analysis:**
  - The dataset contains minimal missing values, with one column having only 1 NaN value and another column with approximately 3.5% NaN values, resulting in an overall total of 3.5% missing values.

- **Duplicate Values Check:**
  - No duplicated values were found within the dataset.

- **Outlier Detection:**
  - Numerous outliers were observed across most columns within the dataset.

- **Skewness Examination:**
  - The majority of columns exhibit positive skewness, with only one column showing approximately normal distribution (purchased frequency), and one column displaying negative skewness (balance frequency,Tenure).

These insights provide a comprehensive overview of the data's quality, highlighting areas for further investigation and potential preprocessing steps.

# ‚öôÔ∏è Data Preprocessing Part-2
"""

df_c=df.copy()

df_c=df_c.dropna()

def TSNE_2D(df, perplexity=40, n_iter=300, hue=None):
  # color=pick_random_color()
  tsne = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=n_iter)
  tsne_results = tsne.fit_transform(df)

  tsne_2d_one = tsne_results[:,0]
  tsne_2d_two = tsne_results[:,1]

  # Plot the embedded data
  plt.figure(figsize=(8, 6))
  sns.scatterplot(x=tsne_2d_one,y=tsne_2d_two,hue=hue)
  plt.title('t-SNE Visualization')
  plt.xlabel('t-SNE Component 1')
  plt.ylabel('t-SNE Component 2')
  plt.show()

TSNE_2D(df_c)

"""## This is our data points without undergoing any preprocessing or transformation. Now, I will apply `Robust Scaler` to mitigate the impact of outliers and reevaluate the TSNE visualization.

# <a name="4">Features transformation</a>
"""

transformer = RobustScaler()
values = transformer.fit_transform(df_c[medium_cols+other_cols])
values

values_df = pd.DataFrame(data=values, columns=transformer.get_feature_names_out())
values_df.head(3)

df_c = df_c.reset_index(drop=True)

df_c = pd.concat((values_df, df_c[low_cols]), axis=1)

df_c.head(4)

TSNE_2D(df_c)

"""### Upon applying Robust Scaler, I observed that some data points have become closer to each other.

# K_Means
"""

from sklearn.cluster import KMeans

wss = []
K = range(2,21)

for k in K:
    kmeans=KMeans(n_clusters=k)
    kmeans=kmeans.fit(df_c)
    wss_iter = kmeans.inertia_
    wss.append(wss_iter)

plt.figure(figsize=(10,6))
plt.plot(K,wss, marker = 'o', c=pick_random_color())
plt.xlabel('K')
plt.ylabel('Within-Cluster-Sum of Squared Errors (WSS) inertia')
plt.xticks(range(int(min(K)), int(max(K)) + 1));

plt.show()

"""> From the graph, I found the best number of clusters is 8 **(using elbow method)**

### The Silhouette Method
The silhouette method measures the similarity of a data point within its cluster. It has a range between +1 and -1 and the higher values denote a good clustering.
"""

sll=[]
k=range(2,22)
for n in k:
  kmeans=KMeans(n_clusters=n).fit(df_c)
  label=kmeans.labels_
  sll.append(metrics.silhouette_score(df_c,label,metric='euclidean'))

plt.figure(figsize=(12,7))
plt.plot(k,sll,marker='o',c=pick_random_color())
plt.xlabel('k')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis For Optimal k')
plt.xticks(K)
plt.show()

"""> The graph indicates that the optimal number of clusters should be 2, as the silhouette score for `k=2` is the highest. However, in our case, the silhouette score doesn't accurately reflect our data due to the shape of the data observed using TSNE. When the data is not easily separable, the silhouette score can be misleading.

"""

kmeans = KMeans(n_clusters=8 ,init="k-means++")
kmeans = kmeans.fit(df_c)
Clusters = kmeans.labels_
Clusters

TSNE_2D(df_c, hue=Clusters)

""">### The clustering results are not satisfactory as there is overlap between classes. Therefore, I will utilize Principal Component Analysis (PCA) on the data to improve the clustering outcome.

# PCA
"""

pca=PCA(n_components=.95)
X2D=pca.fit_transform(df_c)
print(f"Explained Variance Ratio{pca.explained_variance_ratio_} and the sum is {sum(pca.explained_variance_ratio_)}")
# print('Explained variation summation for all components: {}'.format(sum(pca.explained_variance_ratio_)))

X2D.shape

"""### After applying PCA, the number of columns has been reduced from ` 17 to 7 ` while retaining approximately `95%` of the information in the data.

# Elbow Method After PcA
"""

wss = []
K = range(2,21)

for k in K:
    kmeans=KMeans(n_clusters=k)
    kmeans=kmeans.fit(X2D)
    wss_iter = kmeans.inertia_
    wss.append(wss_iter)
k = np.array(K)
plt.figure(figsize=(10,6))
plt.plot(k,wss, marker = 'o', c = pick_random_color())
plt.xlabel('K')
plt.ylabel('Within-Cluster-Sum of Squared Errors (WSS)')
plt.xticks(range(int(min(k)),int(max(k))));

"""> From the graph, I found the best number of clusters is 8 **(using elbow method)**

# KMeans
"""

kmeans = KMeans(n_clusters=8 ,init="k-means++")
kmeans = kmeans.fit(X2D)
Clusters = kmeans.labels_
Clusters

"""# T_SNE"""

tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results=tsne.fit_transform(X2D)
tsne_2d_one = tsne_results[:,0]
tsne_2d_two = tsne_results[:,1]
# Plot the embedded data
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x=tsne_2d_one, y=tsne_2d_two,
    hue=Clusters,
    palette="tab10",
    legend="full"
)
plt.title('t-SNE Visualization')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()

"""### After applying PCA, there has been an improvement in the results. Now, I will attempt another transformation, such as Log transformation, to further enhance the analysis.

# Data Transformation (Log)
"""

df_2c=df.copy()
df_2c.dropna().reset_index(drop=True).head(4)

# to avoid undefined results or issues that might arise from taking the logarithm of zero.
# The reason is that the logarithm of zero is undefined in mathematics. So, if you have zero values in your dataset and you directly apply a log transformation without adding a small constant, it will result in errors or NaN (Not a Number) values.
def change_zero(value):
    if value==0:
        return value+0.001
    return value


for col in df_2c[other_cols + medium_cols]:
    df_2c[col] = np.log(df_2c[col].apply(change_zero))

df_2c

df_2c=df_2c.dropna()
df_2c.head(2)

df_2c.isnull().sum()

skewness = df_2c[columns].skew().sort_values()

plt.figure(figsize=(14,6))
sns.barplot(x=skewness.index, y=skewness, palette=sns.color_palette("Reds",19))
for i, v in enumerate(skewness):
  plt.text(i, v, f"{v:.1f}", ha="center", va="bottom",size=15,fontweight="black")

plt.ylabel("Skewness")
plt.xlabel("Columns")
plt.xticks(rotation=90)
plt.title("Skewness of Continous Numerical Columns",fontweight="black",size=20,pad=10)
plt.tight_layout()
plt.show()

TSNE_2D(df_2c)

"""> After try another transformation (Log transformation),

> the next step:
    - Use Elbow method to determine best k
    - Plot TSNE to see the clusters

# Finding Optimum Value of K

## Elbow Method
"""

ww=[]
n=range(2,22)
for K in n:
  kmeans=KMeans(n_clusters=K)
  kmeans=kmeans.fit(df_2c)
  score=kmeans.inertia_
  ww.append(score)

plt.figure(figsize=(10,6))
plt.plot(n,ww, marker = 'o', c = pick_random_color())
plt.xlabel('K')
plt.ylabel('Within-Cluster-Sum of Squared Errors (WSS)')
plt.xticks(range(int(min(k)),int(max(k))));

"""> From the graph, I found the best number of clusters is 7 **(using elbow method)**

## Silhouette Score
"""

sil = []
K = range(2,21)
for k in K:
  kmeans = KMeans(n_clusters = k).fit(df_2c)
  labels = kmeans.labels_
  sil.append(metrics.silhouette_score(df_2c, labels, metric = 'euclidean'))

plt.plot(K, sil, 'bx-')
plt.xlabel('k')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis For Optimal k')
plt.xticks(K)
plt.show()

"""> From the graph, I found the best number of clusters is 7 **(using silhouette method)**

> In this scenario, the silhouette score aligns with the number of clusters suggested by the elbow method. After applying log transformation and visualizing the data using TSNE, we discovered that the data can be separated, allowing us to estimate the number of clusters from the visualization.

**When data points overlap, the silhouette score can be misleading. As a result, it's common practice to utilize multiple methods such as elbow method, silhouette analysis, hierarchical clustering, etc., along with visualizations to better understand and determine the appropriate number of clusters.**
"""

kmeans = KMeans(n_clusters=7 ,init="k-means++")
kmeans = kmeans.fit(df_2c)
Clusters = kmeans.labels_
TSNE_2D(df_2c, hue=Clusters)

"""### Now that the results are satisfactory, the next step will involve utilizing different methods to determine the optimal number of clusters.

# Hierarchical Clustering
"""

#by analyzing the dendrogram, one can determine the optimal number of clusters or identify meaningful groupings in the data.
from scipy.cluster.hierarchy import dendrogram,linkage
linked=linkage(df_2c,method='single')
plt.figure(figsize=(10,7))
dendrogram(linked,orientation="top", # It determines whether the clustering process proceeds from the bottom up (agglomerative) or from the top down (divisive).
           show_leaf_counts=True,distance_sort="descending")
plt.title('Hierarchical Clustering Dendrogram (Single)')
plt.show()

"""Taking a horizontal line at 6 on the y-axis will result in 7 clusters. There isn't a specific number from which to draw the horizontal line; rather, you can experiment with different values to observe the distribution and number of points in each cluster."""

Z = linkage(df_2c, 'average')

plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram (Average)')
plt.show()

Z = linkage(df_2c, 'complete')

plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram (complete)')
plt.show()

from sklearn.cluster import AgglomerativeClustering
clustering=AgglomerativeClustering(n_clusters=7).fit(df_2c)
cluster=clustering.labels_
TSNE_2D(df_2c,hue=cluster)

"""# DBSCAN"""

#  Unlike k-means or hierarchical clustering, which require specifying the number of clusters beforehand,
#  DBSCAN automatically determines clusters based on the density of data points

from sklearn.cluster import DBSCAN

dbscan=DBSCAN(eps=5,min_samples=500)
labels=dbscan.fit_predict(df_2c)
labels

TSNE_2D(df_2c,hue=labels)

"""> I set the minimum number of data points required to form a cluster to 500 customers with `epsilon = 5`. The resulting number of clusters is 7, which aligns well with the previous techniques and indicates a satisfactory outcome.

# Statistic Method

The **`gap statistic`** is a statistical method utilized to ascertain the optimal number of clusters in a dataset. It evaluates the within-cluster dispersion and contrasts it with the expected dispersion under a suitable reference null distribution of the data.

> Dispersion refers to how spread out or compact the points are within each cluster. Ideally, we seek clusters where the points are closely grouped together.

The gap statistic assesses the spread of points within our actual clusters in comparison to the expected spread if there were no genuine clusters, only random noise.
"""

!pip install gap-stat

from gap_statistic import OptimalK
optimalk=OptimalK(parallel_backend='joblib')
n_clusters=optimalk(df_2c,cluster_array=np.arange(1,11))
print ("Optimal number of clusters:{}".format(n_clusters))

print(f'Optimal number of clusters: {n_clusters}')

plt.figure(figsize=(10, 6))
plt.plot(np.arange(1, 11), optimalk.gap_df.gap_value, linewidth=3, color='b')
plt.scatter(n_clusters, optimalk.gap_df.loc[n_clusters-1, 'gap_value'], s=250, c='red', marker='*', label='Optimal K')
plt.xlabel('Number of Clusters')
plt.ylabel('Gap Value')
plt.title('Gap Statistic')
plt.xticks(np.arange(1, 11))
plt.legend()
plt.grid(True)
plt.show()

"""**After experimenting with various techniques and data transformations, it has been determined that the optimal number of clusters is 7.**

**Next Steps:**
1. Apply KMeans algorithm with 7 clusters.
2. Visualize the data using t-SNE.
3. Perform analysis for each cluster separately, providing insights for each cluster.
"""

kmeans = KMeans(n_clusters=7)
kmeans.fit(df_2c)
labels = kmeans.labels_
df_2c['label'] = labels
df_2c.head(3)

"""### Analysis Data After Clustering
I will conduct an analysis using the original, non-transformed data to provide genuine insights along with relevant statistics.

"""

final_df = df.copy()
final_df = final_df.dropna().reset_index(drop=True)
final_df['label'] = labels
final_df.head(3)

final_df.shape

def cluster_distribution(feature):
  labels = np.unique(final_df['label'])
  fig, ax = plt.subplots(2,4, figsize=(20,12))
  ax = ax.ravel()
  color = pick_random_color()

  for i, label in enumerate(labels):
    if i==len(labels):
      plt.axis('off')
      break
    plt.sca(ax[i])
    filtered_df_label = final_df[final_df['label'] == label]
    sns.histplot(filtered_df_label[feature], kde=True, bins=20, color=color)
    plt.text(0.95, 0.95, f'Mean: {filtered_df_label[feature].mean():.2f}', fontsize=15,
            verticalalignment='top', horizontalalignment='right', bbox=dict(facecolor='white', alpha=0.5))
    plt.title(f'{feature} (Label {label})')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.grid(True)

  plt.tight_layout()

columns = final_df.columns[:-1]
columns

descriptive_stat=final_df.groupby("label").mean()
descriptive_stat

a = descriptive_stat['BALANCE'].to_numpy()
b = descriptive_stat['PURCHASES'].to_numpy()
np.corrcoef(a,b)

sns.scatterplot(data=final_df, x='BALANCE', y='PURCHASES')

"""### There is no correlation between the account balance and purchase behavior."""

cluster_distribution(columns[0])

"""According to the balance within each cluster, we can categorize them into three groups:
1. Groups with an average balance around 2000.
2. Groups with an average balance ranging from 700 to 1000.
3. Groups with an average balance less than 700.
"""

cluster_distribution(columns[1])

"""- We have two clusters with installment purchases having a mean of 0, indicating that they didn't make any installment purchases.
- We can categorize them into 3 groups:
  1. Didn't pay installments.
  2. Paid on average less than 500.
  3. Paid on average over 500 to almost 1000.
"""

descriptive_stat

""" # Analysis Of Variance (ANOVA)"""

#  ANOVA is a technique used to
# compare means across multiple groups to determine whether there are statistically significant differences between them.
from scipy.stats import f_oneway
anova_results={}
for feature in final_df.columns:
    clusters_data = [final_df[feature][final_df['label'] == cluster] for cluster in set(final_df['label'])] #This set operation ensures that each cluster identifier occurs only once

    f_statistic, p_value = f_oneway(*clusters_data)

    anova_results[feature] = {'F-statistic': f_statistic, 'p-value': p_value}

for feature, results in anova_results.items():
    print(f"Feature: {feature}")
    print(f"F-statistic: {results['F-statistic']}, p-value: {results['p-value']}")
    print()

"""> These results indicate that there are significant differences between the clusters in terms of the various features.

**Some insights**:

1. **BALANCE:** The high F-statistic and very low p-value indicate that there are significant differences in the mean balances across clusters. This suggests that different clusters may have different average account balances and we can see that in the previous table.

2. **PURCHASES:** Again, high F-statistic and very low p-value, suggesting significant differences in the mean purchase amounts across clusters. This implies that different clusters may have customers with varying purchasing behaviors.

3. **ONEOFF_PURCHASES:** Similar to PURCHASES, indicating significant differences in the mean amounts of one-off purchases across clusters.

4. **INSTALLMENTS_PURCHASES:** Similar to PURCHASES, indicating significant differences in the mean amounts of installment purchases across clusters.

5. **CASH_ADVANCE:** High F-statistic and very low p-value, indicating significant differences in the mean cash advance amounts across clusters. This suggests that some clusters may have customers who use cash advances more frequently or in larger amounts.

Anova benfit For example, in a medical study comparing the effectiveness of different treatments, knowing if there are significant differences in outcomes can help clinicians choose the most effective treatment for their patients.
"""

clusters_count = final_df.groupby('label')['BALANCE'].count().to_frame().reset_index()
clusters_count

sns.barplot(data=clusters_count,x="label",y=clusters_count['BALANCE'])

from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
# Instantiate the KElbowVisualizer with the number of clusters range and the scoring metric
visualizer = KElbowVisualizer(model, k=(2,10), metric='distortion')
# Fit the data and visualize
visualizer.fit(df_2c)
visualizer.show()



"""# IMPORTANT*** QA***

t-SNE:
 used for visualization and dimensionality reduction of high-dimensional data. It is particularly effective for visualizing complex, high-dimensional datasets in a lower-dimensional space (typically 2D or 3D) while preserving the structure and relationships between data points as much as possible.

The algorithm works by modeling each high-dimensional data point with a probability distribution in the lower-dimensional space, such that similar data points in the high-dimensional space are modeled with higher probabilities of being close to each other in the lower-dimensional space, and dissimilar points are modeled with lower probabilities of being close.

t-SNE optimizes these probability distributions by minimizing the difference between the original high-dimensional distances and the distances between the modeled data points in the lower-dimensional space, typically using gradient descent optimization techniques.

LLE stands for Locally Linear Embedding. It's a technique in machine learning used for non-linear dimensionality reduction. Like t-SNE, LLE is used for visualizing high-dimensional data in lower dimensions while preserving the local structure of the data.

The main idea behind LLE is to find a lower-dimensional representation of the data while preserving the relationships between neighboring data points. It does so by first constructing a graph that represents the local structure of the data, typically by connecting each data point to its nearest neighbors. Then, for each data point, it reconstructs the point as a linear combination of its neighbors, aiming to preserve these local relationships

The steps involved in LLE are as follows:

Local Neighborhood Identification: Identify the k nearest neighbors for each data point in the high-dimensional space.

Local Reconstruction Weighting: For each data point, find the weights that best reconstruct the point as a linear combination of its neighbors. This is done by solving a linear equation system.

Global Coordinate Mapping: Use the weights obtained in the previous step to map the data points to a lower-dimensional space while preserving the local relationships as much as possible.

LLE tends to work well when the data lies on or near a low-dimensional manifold within the high-dimensional space. It's particularly useful for tasks such as manifold learning, where the goal is to uncover the underlying structure of the data.

A Gaussian Mixture Model (GMM) :  is a probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters. Each Gaussian distribution represents a cluster in the data, and the mixture model specifies the probability that each data point belongs to each of these clusters.

When we talk about "soft clustering" in the context of Gaussian Mixture Models, it means that instead of assigning each data point to a single cluster, the model assigns each data point a probability or likelihood of belonging to each cluster. This is in contrast to "hard clustering," where each data point is assigned exclusively to one cluster.

Here's how Gaussian Mixture Soft Clustering works:

Initialization: Initially, the parameters of the Gaussian distributions (mean, covariance, and mixing coefficients) are randomly initialized or set using some other method.

Expectation-Maximization (EM) Algorithm: The EM algorithm is used to iteratively update the parameters of the Gaussian distributions to maximize the likelihood of the observed data. In the E-step, the algorithm calculates the probability or likelihood of each data point belonging to each cluster based on the current parameters. In the M-step, the parameters are updated based on these probabilities.

Convergence: The EM algorithm iterates between the E-step and M-step until convergence is reached, meaning that the parameters no longer change significantly between iterations.

Cluster Assignment: After convergence, each data point has a probability distribution over the clusters. To obtain hard cluster assignments, we typically assign each data point to the cluster with the highest probability.

The Silhouette score: is a metric used to evaluate the quality of clusters created by clustering algorithms. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). A higher silhouette score indicates that the object is well-matched to its own cluster and poorly matched to neighboring clusters.

The silhouette score ranges from -1 to 1, where:

A score close to +1 indicates that the object is well-clustered and lies far from neighboring clusters.
A score close to 0 indicates that the object is close to the decision boundary between two neighboring clusters.
A score close to -1 indicates that the object may have been assigned to the wrong cluster.
The silhouette score is calculated for each data point and then averaged to obtain the overall silhouette score for the clustering. It provides a measure of how compact and well-separated the clusters are in the data space

RobustScaler : is a method used for scaling features to be centered around the median and have a certain interquartile range (IQR). It's robust to outliers, meaning it doesn't get influenced by them as much as other scaling methods like StandardScaler, which uses the mean and standard deviation.

Here's a brief overview of how RobustScaler works:

It computes the median and the interquartile range (IQR) for each feature.
It then scales each feature by subtracting the median and dividing by the IQR.
This scaling method is particularly useful when dealing with datasets that contain outliers or when the data does not follow a normal distribution

Hierarchical clustering is a type of unsupervised machine learning algorithm used to group together similar objects or data points in a hierarchical manner. The algorithm builds a tree-like structure of clusters, where each node in the tree represents a cluster. At the lowest level of the hierarchy, each data point is considered a separate cluster. As we move up the hierarchy, these clusters are merged together based on their similarity until all the data points belong to a single cluster at the root of the tree.

A dendrogram is a visual representation of the hierarchical clustering process. It is a tree-like diagram that illustrates the arrangement of the clusters produced by the algorithm. In a dendrogram, each branch represents a cluster, and the height of the branch represents the distance or dissimilarity between the clusters being merged. The longer the branch, the greater the dissimilarity between the clusters being merged. By analyzing the dendrogram, one can determine the optimal number of clusters or identify meaningful groupings in the data.

Linkage is a crucial concept in hierarchical clustering algorithms. It refers to the method used to measure the distance or dissimilarity between clusters when deciding which clusters to merge at each step of the algorithm. There are several linkage methods commonly used in hierarchical clustering:

Single linkage: In single linkage, the distance between two clusters is defined as the shortest distance between any two points in the two clusters. This method tends to produce long, "chain-like" clusters.

Complete linkage: In complete linkage, the distance between two clusters is defined as the maximum distance between any two points in the two clusters. This method tends to produce compact, spherical clusters.

Average linkage: In average linkage, the distance between two clusters is defined as the average distance between all pairs of points in the two clusters. This method strikes a balance between single and complete linkage and often produces balanced clusters.

Ward's linkage: In Ward's linkage, the distance between two clusters is defined as the increase in the squared error that would result from merging the two clusters. This method tends to minimize the variance within each cluster and is often used for clustering when the goal is to minimize intra-cluster variance.

Agglomerative Hierarchical Clustering:

This approach starts with each data point as a separate cluster and iteratively merges the closest clusters together until all points belong to a single cluster. It is often referred to as "bottom-up" clustering because it starts from the leaves (individual data points) and moves towards the root (the single cluster containing all points).
In this approach, the orientation hyperparameter isn't explicitly specified because the algorithm inherently follows a bottom-up approach.
Divisive Hierarchical Clustering:

This approach begins with all data points grouped into a single cluster and then splits clusters recursively until each data point is in its own cluster. It is often referred to as "top-down" clustering because it starts from the root (the single cluster containing all points) and moves towards the leaves (individual data points).
In this approach, the orientation hyperparameter might be explicitly specified to indicate the top-down nature of the clustering process.

DBSCAN, which stands for Density-Based Spatial Clustering of Applications with Noise, is a popular clustering algorithm in machine learning and data mining. Unlike hierarchical clustering or k-means clustering, DBSCAN does not require specifying the number of clusters beforehand and can identify clusters of arbitrary shapes.

Here's how DBSCAN works:

Density-Based Clustering:

DBSCAN groups together closely packed points based on two parameters: epsilon (Œµ), which defines the radius of the neighborhood around a point, and minPoints, which specifies the minimum number of points required to form a dense region.
A point is considered a core point if within its Œµ-neighborhood there are at least minPoints other points (including itself).
A point is considered a border point if it's within the Œµ-neighborhood of a core point but does not meet the minPoints criterion itself.
Any point that is neither a core nor a border point is considered noise.
Algorithm Steps:

The algorithm starts by selecting an arbitrary point from the dataset.
If the selected point is a core point, DBSCAN recursively expands a cluster around it by adding all reachable points within Œµ distance.
If the selected point is a border point, it is assigned to the cluster of one of its neighboring core points.
The algorithm continues until all points have been assigned to a cluster or labeled as noise.
Resulting Clusters:

The clusters formed by DBSCAN can vary in size and shape and are not required to be convex.
DBSCAN is capable of handling noise points, as they are not assigned to any cluster but labeled as outliers.
DBSCAN has several advantages:

It can identify clusters of arbitrary shapes and sizes.
It does not require specifying the number of clusters beforehand.
It is robust to outliers and can handle noise well.
It is relatively efficient, especially for large datasets, as it does not need to compute distances between all pairs of points.
However, DBSCAN also has some limitations:

It may struggle with datasets of varying densities or clusters with significantly different densities.
Determining the optimal values for the epsilon (Œµ) and minPoints parameters can be challenging and may require domain knowledge or trial and error.
It may not perform well in high-dimensional spaces due to the curse of dimensionality.

f_oneway: is a **function used in statistical analysis**, particularly in the **context of analysis of variance (ANOVA)**. ANOVA is a technique used **to compare means across multiple groups to determine whether there are statistically significant differences between them.**
"""















































